seed = 42
fast_dev_run = false
regenerate = true
tune = false
train = true
evaluate = false
model_id = "answerdotai/ModernBERT-large"
verbose = true

[dataset]
in_path = "data/raw/"
inter_path = "data/intermediate/"
out_path = "data/processed/"
train_size = 0.8
max_length = 128
chunk_length = 128
overlap = 32

[dataloader]
batch_size = 16
pin_memory = true
num_workers = 0

[database]
path = "data/db/"
num_partitions = 256
num_sub_vectors = 96
k_neighbors = 8

[model]
num_queries = 16
d_model = 0              # inferred at run time
nhead = 1
dim_feedforward = 256
dropout = 0.1
output_dim = 0           # inferred at run time
query_ini_random = false

[optimizer]
lr = 1e-4
weight_decay = 1e-2

[trainer]
max_epochs = 20
save_path = "models/hyperpartisan.pt"
gradient_clip = 1.0
ignore_index = -100
eval_every_n_epochs = 1
temperature = 1.0

[tuner]
path = "models/hyperparameters.json"
checkpoint = "models/hyperpartisan.pt"
n_trials = 10
prune = false

[hparams]
[hparams.max_epochs]
name = "max_epochs"
low = 2
high = 8
[hparams.lr]
name = "lr"
low = 1e-5
high = 1e-3
[hparams.weight_decay]
name = "weight_decay"
low = 1e-5
high = 1e-2
[hparams.temperature]
name = "temperature"
low = 0.1
high = 2.0

[evaluator]
path = "data/results.parquet"
n_bootstraps = 100
